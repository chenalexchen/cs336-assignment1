{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ccd6edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 XL parameter count: 2127057600\n",
      "GPT-2 XL parameter size with float32: 8508230400\n"
     ]
    }
   ],
   "source": [
    "def trainable_parameter_count_transformer_block(d_model: int, d_ff: int):\n",
    "    return 4 * d_model * d_model + 3 * d_model * d_ff + 2 * d_model\n",
    "\n",
    "def trainable_parameter_count_transformer_lm(vocab_size: int, context_length: int, num_layers: int,\n",
    "                                             d_model: int, num_heads: int, d_ff: int):\n",
    "    count = 0\n",
    "    count += 2 * vocab_size * d_model  # Token embedding + Output linear layer\n",
    "    count += d_model  # RMSNorm for output\n",
    "    count += trainable_parameter_count_transformer_block(d_model, d_ff) * num_layers\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "gpt2_xl_param_count = trainable_parameter_count_transformer_lm(vocab_size=50257, context_length=1024,\n",
    "                                                               num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
    "    \n",
    "print('GPT-2 XL parameter count: ' + str(gpt2_xl_param_count))\n",
    "print('GPT-2 XL parameter size with float32: ' + str(gpt2_xl_param_count * 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2w2jnenz72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# FLOP Calculation Functions\n",
    "# ===============================================\n",
    "\n",
    "def compute_attention_flops(batch_size: int, seq_len: int, d_model: int, num_heads: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for multi-head self-attention.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with FLOP counts for each attention operation\n",
    "    \"\"\"\n",
    "    head_dim = d_model // num_heads\n",
    "    \n",
    "    # QKV projections: 3 linear layers of [batch, seq_len, d_model] x [d_model, d_model]\n",
    "    qkv_flops = 3 * 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    # Attention scores: Q x K^T for each head\n",
    "    # Q, K shape per head: [batch, heads, seq_len, head_dim]\n",
    "    attention_scores_flops = 2 * batch_size * num_heads * seq_len * seq_len * head_dim\n",
    "    \n",
    "    # Attention output: AttentionWeights x V for each head\n",
    "    attention_output_flops = 2 * batch_size * num_heads * seq_len * seq_len * head_dim\n",
    "    \n",
    "    # Output projection: [batch, seq_len, d_model] x [d_model, d_model]\n",
    "    output_proj_flops = 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    return {\n",
    "        'qkv_projections': qkv_flops,\n",
    "        'attention_scores': attention_scores_flops,\n",
    "        'attention_output': attention_output_flops,\n",
    "        'output_projection': output_proj_flops,\n",
    "        'total_attention': qkv_flops + attention_scores_flops + attention_output_flops + output_proj_flops\n",
    "    }\n",
    "\n",
    "def compute_ffn_flops(batch_size: int, seq_len: int, d_model: int, d_ff: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for SwiGLU feed-forward network.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "        d_ff: Feed-forward dimension\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with FLOP counts for each FFN operation\n",
    "    \"\"\"\n",
    "    # W1 projection: [batch, seq_len, d_model] x [d_model, d_ff]\n",
    "    w1_flops = 2 * batch_size * seq_len * d_model * d_ff\n",
    "    \n",
    "    # W3 projection: [batch, seq_len, d_model] x [d_model, d_ff]\n",
    "    w3_flops = 2 * batch_size * seq_len * d_model * d_ff\n",
    "    \n",
    "    # W2 projection: [batch, seq_len, d_ff] x [d_ff, d_model]\n",
    "    w2_flops = 2 * batch_size * seq_len * d_ff * d_model\n",
    "    \n",
    "    return {\n",
    "        'w1_projection': w1_flops,\n",
    "        'w3_projection': w3_flops, \n",
    "        'w2_projection': w2_flops,\n",
    "        'total_ffn': w1_flops + w3_flops + w2_flops\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9mkftazoq2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformer_layer_flops(batch_size: int, seq_len: int, d_model: int, \n",
    "                                   num_heads: int, d_ff: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for a single transformer layer.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with FLOP counts for the transformer layer\n",
    "    \"\"\"\n",
    "    attention_flops = compute_attention_flops(batch_size, seq_len, d_model, num_heads)\n",
    "    ffn_flops = compute_ffn_flops(batch_size, seq_len, d_model, d_ff)\n",
    "    \n",
    "    # RMSNorm operations are relatively cheap (just element-wise ops), so we focus on matrix multiplies\n",
    "    \n",
    "    return {\n",
    "        'attention': attention_flops['total_attention'],\n",
    "        'ffn': ffn_flops['total_ffn'],\n",
    "        'total_layer': attention_flops['total_attention'] + ffn_flops['total_ffn'],\n",
    "        'attention_breakdown': attention_flops,\n",
    "        'ffn_breakdown': ffn_flops\n",
    "    }\n",
    "\n",
    "def compute_transformer_lm_flops(batch_size: int, seq_len: int, vocab_size: int,\n",
    "                                d_model: int, num_layers: int, num_heads: int, \n",
    "                                d_ff: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for the complete transformer language model.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        vocab_size: Vocabulary size\n",
    "        d_model: Model dimension\n",
    "        num_layers: Number of transformer layers\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with complete FLOP breakdown\n",
    "    \"\"\"\n",
    "    # Token embedding is just indexing (no matrix multiply)\n",
    "    embedding_flops = 0\n",
    "    \n",
    "    # Single layer FLOPs\n",
    "    layer_flops = compute_transformer_layer_flops(batch_size, seq_len, d_model, num_heads, d_ff)\n",
    "    \n",
    "    # All layers\n",
    "    all_layers_flops = num_layers * layer_flops['total_layer']\n",
    "    \n",
    "    # Language model head: [batch, seq_len, d_model] x [d_model, vocab_size]\n",
    "    lm_head_flops = 2 * batch_size * seq_len * d_model * vocab_size\n",
    "    \n",
    "    # Total FLOPs\n",
    "    total_flops = embedding_flops + all_layers_flops + lm_head_flops\n",
    "    \n",
    "    return {\n",
    "        'embedding': embedding_flops,\n",
    "        'per_layer': layer_flops['total_layer'],\n",
    "        'all_layers': all_layers_flops,\n",
    "        'lm_head': lm_head_flops,\n",
    "        'total': total_flops,\n",
    "        'layer_breakdown': layer_flops\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8wdviveb4on",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_flops(flops: int) -> str:\n",
    "    \"\"\"\n",
    "    Format FLOP count in human-readable format.\n",
    "    \n",
    "    Args:\n",
    "        flops: Number of FLOPs\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with appropriate units\n",
    "    \"\"\"\n",
    "    if flops >= 1e12:\n",
    "        return f\"{flops/1e12:.1f}T\"\n",
    "    elif flops >= 1e9:\n",
    "        return f\"{flops/1e9:.1f}B\"\n",
    "    elif flops >= 1e6:\n",
    "        return f\"{flops/1e6:.1f}M\"\n",
    "    elif flops >= 1e3:\n",
    "        return f\"{flops/1e3:.1f}K\"\n",
    "    else:\n",
    "        return str(flops)\n",
    "\n",
    "def print_flop_breakdown(flop_dict, title: str = \"FLOP Breakdown\"):\n",
    "    \"\"\"\n",
    "    Print a nicely formatted FLOP breakdown.\n",
    "    \n",
    "    Args:\n",
    "        flop_dict: Dictionary with FLOP counts\n",
    "        title: Title for the breakdown\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\" * len(title + \":\"))\n",
    "    \n",
    "    for key, value in flop_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Skip nested breakdown dictionaries in the main view\n",
    "            if 'breakdown' in key:\n",
    "                continue\n",
    "            print(f\"{key.replace('_', ' ').title()}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                if isinstance(sub_value, (int, float)):\n",
    "                    print(f\"  {sub_key.replace('_', ' ').title()}: {format_flops(sub_value)} ({sub_value:,})\")\n",
    "        else:\n",
    "            print(f\"{key.replace('_', ' ').title()}: {format_flops(value)} ({value:,})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dytjc94elf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_breakdown(flop_dict, title: str = \"Detailed FLOP Breakdown\"):\n",
    "    \"\"\"\n",
    "    Print a detailed FLOP breakdown including all nested components.\n",
    "    \n",
    "    Args:\n",
    "        flop_dict: Dictionary with FLOP counts\n",
    "        title: Title for the breakdown\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\" * len(title + \":\"))\n",
    "    \n",
    "    # Main totals first\n",
    "    for key, value in flop_dict.items():\n",
    "        if not isinstance(value, dict) and 'breakdown' not in key:\n",
    "            print(f\"{key.replace('_', ' ').title()}: {format_flops(value)} ({value:,})\")\n",
    "    \n",
    "    # Then detailed breakdowns\n",
    "    if 'layer_breakdown' in flop_dict:\n",
    "        layer_breakdown = flop_dict['layer_breakdown']\n",
    "        print(f\"\\nPer-Layer Breakdown:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        if 'attention_breakdown' in layer_breakdown:\n",
    "            print(\"  Attention Components:\")\n",
    "            for k, v in layer_breakdown['attention_breakdown'].items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    print(f\"    {k.replace('_', ' ').title()}: {format_flops(v)} ({v:,})\")\n",
    "        \n",
    "        if 'ffn_breakdown' in layer_breakdown:\n",
    "            print(\"  FFN Components:\")\n",
    "            for k, v in layer_breakdown['ffn_breakdown'].items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    print(f\"    {k.replace('_', ' ').title()}: {format_flops(v)} ({v:,})\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5xgi4g2ffug",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 XL FLOP Breakdown:\n",
      "========================\n",
      "Embedding: 0 (0)\n",
      "Per Layer: 90.6B (90,596,966,400)\n",
      "All Layers: 4.3T (4,348,654,387,200)\n",
      "Lm Head: 164.7B (164,682,137,600)\n",
      "Total: 4.5T (4,513,336,524,800)\n",
      "\n",
      "Total FLOPs: 4.5T\n",
      "FLOPs per token: 4.4B\n",
      "\n",
      "GPT-2 XL Detailed Breakdown:\n",
      "============================\n",
      "Embedding: 0 (0)\n",
      "Per Layer: 90.6B (90,596,966,400)\n",
      "All Layers: 4.3T (4,348,654,387,200)\n",
      "Lm Head: 164.7B (164,682,137,600)\n",
      "Total: 4.5T (4,513,336,524,800)\n",
      "\n",
      "Per-Layer Breakdown:\n",
      "--------------------\n",
      "  Attention Components:\n",
      "    Qkv Projections: 15.7B (15,728,640,000)\n",
      "    Attention Scores: 3.4B (3,355,443,200)\n",
      "    Attention Output: 3.4B (3,355,443,200)\n",
      "    Output Projection: 5.2B (5,242,880,000)\n",
      "    Total Attention: 27.7B (27,682,406,400)\n",
      "  FFN Components:\n",
      "    W1 Projection: 21.0B (20,971,520,000)\n",
      "    W3 Projection: 21.0B (20,971,520,000)\n",
      "    W2 Projection: 21.0B (20,971,520,000)\n",
      "    Total Ffn: 62.9B (62,914,560,000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Example: GPT-2 XL FLOP Analysis\n",
    "# ===============================================\n",
    "\n",
    "# GPT-2 XL Configuration\n",
    "gpt2_xl_config = {\n",
    "    'batch_size': 1,\n",
    "    'seq_len': 1024,\n",
    "    'vocab_size': 50257,\n",
    "    'd_model': 1600,\n",
    "    'num_layers': 48,\n",
    "    'num_heads': 25,\n",
    "    'd_ff': 6400\n",
    "}\n",
    "\n",
    "# Compute FLOPs for GPT-2 XL\n",
    "gpt2_xl_flops = compute_transformer_lm_flops(**gpt2_xl_config)\n",
    "\n",
    "# Print breakdown\n",
    "print_flop_breakdown(gpt2_xl_flops, \"GPT-2 XL FLOP Breakdown\")\n",
    "\n",
    "print(f\"Total FLOPs: {format_flops(gpt2_xl_flops['total'])}\")\n",
    "print(f\"FLOPs per token: {format_flops(gpt2_xl_flops['total'] // gpt2_xl_config['seq_len'])}\")\n",
    "\n",
    "# Show detailed component breakdown\n",
    "print_detailed_breakdown(gpt2_xl_flops, \"GPT-2 XL Detailed Breakdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
