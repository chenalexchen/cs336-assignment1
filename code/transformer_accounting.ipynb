{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "oln3tj60dk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY OF RESULTS\n",
      "====================\n",
      "\n",
      "(a) Algebraic expressions for AdamW memory usage:\n",
      "   Parameters:     4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\n",
      "   Activations:    4 * B*T*(H + L*(7*H + A*T + 2*d_ff) + V) bytes (essential only)\n",
      "   Gradients:      4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\n",
      "   Optimizer:      8 * [2*V*H + L*(16*H² + 2*H) + H] bytes\n",
      "   TOTAL:         16*[2*V*H + L*(16*H² + 2*H) + H] + 4*B*T*(H + L*(7*H + A*T + 2*d_ff) + V)\n",
      "\n",
      "(b) GPT-2 XL memory expression:\n",
      "   Memory (GB) = 10.285 * batch_size + 34.0\n",
      "   Maximum batch size for 80GB: 4\n",
      "\n",
      "(c) AdamW FLOPs:\n",
      "   14 * [number of parameters] FLOPs per optimization step\n",
      "   For transformer: 14 * [2*V*H + L*(16*H² + 2*H) + H]\n",
      "\n",
      "(d) Training time:\n",
      "   GPT-2 XL training for 400K steps at batch size 1024: 6583.6 days\n",
      "   Justification: 13.2B FLOPs/token × 419B tokens ÷ 9.75 TFLOPs/s = 565 days\n",
      "\n",
      "Detailed calculations:\n",
      "Total parameters: 2,127,057,600\n",
      "Batch-independent memory: 34.03 GB\n",
      "Essential activations per token: 2,511,057\n",
      "Activation coefficient: 10.285 GB per batch item\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Summary and Verification\n",
    "# ===============================================\n",
    "\n",
    "# Calculate GPT-2 XL constants for summary\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "d_ff = 6400\n",
    "\n",
    "# Calculate batch-size independent terms (CORRECTED)\n",
    "total_params = 2 * vocab_size * d_model + num_layers * (16 * d_model**2 + 2 * d_model) + d_model\n",
    "# Parameters (4 bytes) + Gradients (4 bytes) + Optimizer state (8 bytes) = 16 bytes per parameter\n",
    "param_grad_opt_memory = total_params * 16  # NOT 16 * 4!\n",
    "\n",
    "# Calculate batch-size dependent coefficient (more conservative estimate)\n",
    "# Only count essential activations needed for backpropagation\n",
    "essential_activations_per_token = (\n",
    "    d_model +  # token embedding output\n",
    "    num_layers * (\n",
    "        2 * d_model +  # 2 RMSNorm outputs per layer\n",
    "        3 * d_model +  # QKV projections \n",
    "        num_heads * context_length +  # attention scores (simplified)\n",
    "        2 * d_model +  # attention output + projection\n",
    "        2 * d_ff +     # W1, W3 outputs\n",
    "        d_model        # W2 output\n",
    "    ) +\n",
    "    d_model +  # final layer norm\n",
    "    vocab_size # output logits\n",
    ")\n",
    "\n",
    "activation_coeff = essential_activations_per_token * context_length * 4  # 4 bytes per float32\n",
    "\n",
    "# Convert to GB\n",
    "a = activation_coeff / 1e9  # GB per batch item\n",
    "b = param_grad_opt_memory / 1e9  # GB batch-independent\n",
    "\n",
    "# Calculate max batch size for 80GB\n",
    "memory_limit_gb = 80\n",
    "max_batch_size = int((memory_limit_gb - b) / a)\n",
    "\n",
    "# Calculate training time (CORRECTED)\n",
    "num_steps = 400_000\n",
    "batch_size_train = 1024\n",
    "mfu = 0.50  # 50% Model FLOPs Utilization\n",
    "a100_peak_flops = 19.5e12  # 19.5 TFLOPs/s\n",
    "forward_flops_total = 4_513_336_524_800  # From earlier FLOP analysis\n",
    "forward_flops_per_token = forward_flops_total / context_length\n",
    "# Forward + Backward (2x forward) = 3x forward FLOPs per token\n",
    "total_flops_per_token = 3 * forward_flops_per_token\n",
    "total_tokens = num_steps * batch_size_train * context_length\n",
    "total_training_flops = total_tokens * total_flops_per_token\n",
    "effective_throughput = a100_peak_flops * mfu\n",
    "training_time_days = total_training_flops / effective_throughput / 86400\n",
    "\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\" * 20)\n",
    "print()\n",
    "\n",
    "print(\"(a) Algebraic expressions for AdamW memory usage:\")\n",
    "print(\"   Parameters:     4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\")\n",
    "print(\"   Activations:    4 * B*T*(H + L*(7*H + A*T + 2*d_ff) + V) bytes (essential only)\")\n",
    "print(\"   Gradients:      4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\")\n",
    "print(\"   Optimizer:      8 * [2*V*H + L*(16*H² + 2*H) + H] bytes\")\n",
    "print(\"   TOTAL:         16*[2*V*H + L*(16*H² + 2*H) + H] + 4*B*T*(H + L*(7*H + A*T + 2*d_ff) + V)\")\n",
    "print()\n",
    "\n",
    "print(f\"(b) GPT-2 XL memory expression:\")\n",
    "print(f\"   Memory (GB) = {a:.3f} * batch_size + {b:.1f}\")\n",
    "print(f\"   Maximum batch size for 80GB: {max_batch_size}\")\n",
    "print()\n",
    "\n",
    "print(\"(c) AdamW FLOPs:\")\n",
    "print(\"   14 * [number of parameters] FLOPs per optimization step\")\n",
    "print(\"   For transformer: 14 * [2*V*H + L*(16*H² + 2*H) + H]\")\n",
    "print()\n",
    "\n",
    "print(f\"(d) Training time:\")\n",
    "print(f\"   GPT-2 XL training for 400K steps at batch size 1024: {training_time_days:.1f} days\")\n",
    "print(\"   Justification: 13.2B FLOPs/token × 419B tokens ÷ 9.75 TFLOPs/s = 565 days\")\n",
    "\n",
    "# Detailed breakdown for verification\n",
    "print(f\"\\nDetailed calculations:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Batch-independent memory: {b:.2f} GB\")\n",
    "print(f\"Essential activations per token: {essential_activations_per_token:,}\")\n",
    "print(f\"Activation coefficient: {a:.3f} GB per batch item\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "msfthihu74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING TIME ANALYSIS\n",
      "=========================\n",
      "Configuration:\n",
      "- Training steps: 400,000\n",
      "- Batch size: 1,024\n",
      "- Context length: 1,024\n",
      "- MFU: 50%\n",
      "- A100 peak FLOPs: 19.5 TFLOPs/s\n",
      "\n",
      "Forward pass FLOPs per token: 4407.6M\n",
      "Forward pass FLOPs per batch: 4621.66T\n",
      "Backward pass FLOPs per token: 8815.1M\n",
      "Total FLOPs per token (forward + backward): 13222.7M\n",
      "\n",
      "Training computation:\n",
      "Total tokens processed: 419,430,400,000 = 419.43B tokens\n",
      "Total FLOPs: 5545987.92 petaFLOPs\n",
      "\n",
      "Training time calculation:\n",
      "Effective throughput: 9.75 TFLOPs/s\n",
      "Training time: 568.82M seconds\n",
      "Training time: 158,005 hours\n",
      "Training time: 6583.6 days\n",
      "\n",
      "JUSTIFICATION:\n",
      "- Forward pass: 4.4B FLOPs per token (from earlier analysis)\n",
      "- Backward pass: 2× forward = 8.8B FLOPs per token\n",
      "- Total: 13.2B FLOPs per token\n",
      "- Training processes 419B tokens\n",
      "- Total computation: 5546.0 exaFLOPs\n",
      "- At 50% MFU on A100: 9.8 TFLOPs/s effective\n",
      "- Time = 5546.0 EFLOPs ÷ 9.8 TFLOPs/s = 6583.6 days\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# (d) Model FLOPs Utilization (MFU) and Training Time\n",
    "# ===============================================\n",
    "\n",
    "def analyze_training_time():\n",
    "    \"\"\"\n",
    "    Calculate training time for GPT-2 XL with given constraints.\n",
    "    \"\"\"\n",
    "    print(\"TRAINING TIME ANALYSIS\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # Training parameters\n",
    "    num_steps = 400_000\n",
    "    batch_size = 1024\n",
    "    context_length = 1024\n",
    "    mfu = 0.50  # 50% Model FLOPs Utilization\n",
    "    a100_peak_flops = 19.5e12  # 19.5 teraFLOPs per second\n",
    "    \n",
    "    # GPT-2 XL configuration\n",
    "    vocab_size = 50257\n",
    "    num_layers = 48\n",
    "    d_model = 1600\n",
    "    num_heads = 25\n",
    "    d_ff = 6400\n",
    "    \n",
    "    print(\"Configuration:\")\n",
    "    print(f\"- Training steps: {num_steps:,}\")\n",
    "    print(f\"- Batch size: {batch_size:,}\")\n",
    "    print(f\"- Context length: {context_length:,}\")\n",
    "    print(f\"- MFU: {mfu:.0%}\")\n",
    "    print(f\"- A100 peak FLOPs: {a100_peak_flops/1e12:.1f} TFLOPs/s\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate forward pass FLOPs per token (from previous analysis)\n",
    "    # From earlier calculation: GPT-2 XL forward pass = 4.51T FLOPs for batch_size=1, seq_len=1024\n",
    "    forward_flops_total = 4_513_336_524_800  # From earlier calculation\n",
    "    forward_flops_per_token = forward_flops_total / (1 * context_length)\n",
    "    \n",
    "    print(f\"Forward pass FLOPs per token: {forward_flops_per_token/1e6:.1f}M\")\n",
    "    print(f\"Forward pass FLOPs per batch: {forward_flops_per_token * batch_size * context_length/1e12:.2f}T\")\n",
    "    \n",
    "    # Backward pass has 2x forward pass FLOPs\n",
    "    backward_flops_per_token = 2 * forward_flops_per_token\n",
    "    total_flops_per_token = forward_flops_per_token + backward_flops_per_token  # 3x forward\n",
    "    \n",
    "    print(f\"Backward pass FLOPs per token: {backward_flops_per_token/1e6:.1f}M\")\n",
    "    print(f\"Total FLOPs per token (forward + backward): {total_flops_per_token/1e6:.1f}M\")\n",
    "    print()\n",
    "    \n",
    "    # Total FLOPs for entire training\n",
    "    total_tokens = num_steps * batch_size * context_length\n",
    "    total_training_flops = total_tokens * total_flops_per_token\n",
    "    \n",
    "    print(\"Training computation:\")\n",
    "    print(f\"Total tokens processed: {total_tokens:,} = {total_tokens/1e9:.2f}B tokens\")\n",
    "    print(f\"Total FLOPs: {total_training_flops/1e15:.2f} petaFLOPs\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate training time\n",
    "    effective_throughput = a100_peak_flops * mfu  # FLOPs/s at 50% MFU\n",
    "    training_time_seconds = total_training_flops / effective_throughput\n",
    "    training_time_hours = training_time_seconds / 3600\n",
    "    training_time_days = training_time_hours / 24\n",
    "    \n",
    "    print(\"Training time calculation:\")\n",
    "    print(f\"Effective throughput: {effective_throughput/1e12:.2f} TFLOPs/s\")\n",
    "    print(f\"Training time: {training_time_seconds/1e6:.2f}M seconds\")\n",
    "    print(f\"Training time: {training_time_hours:,.0f} hours\") \n",
    "    print(f\"Training time: {training_time_days:.1f} days\")\n",
    "    print()\n",
    "    \n",
    "    print(\"JUSTIFICATION:\")\n",
    "    print(\"- Forward pass: 4.4B FLOPs per token (from earlier analysis)\")\n",
    "    print(\"- Backward pass: 2× forward = 8.8B FLOPs per token\")\n",
    "    print(\"- Total: 13.2B FLOPs per token\")\n",
    "    print(f\"- Training processes {total_tokens/1e9:.0f}B tokens\")\n",
    "    print(f\"- Total computation: {total_training_flops/1e18:.1f} exaFLOPs\")\n",
    "    print(f\"- At 50% MFU on A100: {effective_throughput/1e12:.1f} TFLOPs/s effective\")\n",
    "    print(f\"- Time = {total_training_flops/1e18:.1f} EFLOPs ÷ {effective_throughput/1e12:.1f} TFLOPs/s = {training_time_days:.1f} days\")\n",
    "    \n",
    "    return training_time_days\n",
    "\n",
    "training_days = analyze_training_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1u43nn0mzx2j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAMW FLOPS ANALYSIS\n",
      "=========================\n",
      "AdamW performs the following operations per parameter:\n",
      "\n",
      "For each parameter p with gradient g:\n",
      "1. Weight decay: p ← p * (1 - lr * weight_decay)\n",
      "   - 1 multiplication per parameter\n",
      "\n",
      "2. Update first moment: exp_avg ← β₁ * exp_avg + (1-β₁) * g\n",
      "   - 3 operations per parameter (2 multiplications + 1 addition)\n",
      "\n",
      "3. Update second moment: exp_avg_sq ← β₂ * exp_avg_sq + (1-β₂) * g²\n",
      "   - 4 operations per parameter (3 multiplications + 1 addition)\n",
      "\n",
      "4. Bias correction:\n",
      "   - bias_correction1 = 1 - β₁^step\n",
      "   - bias_correction2 = 1 - β₂^step\n",
      "   - These are scalars computed once per step (negligible)\n",
      "\n",
      "5. Parameter update: p ← p - (lr / bias_correction1) * exp_avg / (√(exp_avg_sq / bias_correction2) + ε)\n",
      "   - Square root: 1 operation per parameter\n",
      "   - Division by bias_correction2: 1 operation per parameter\n",
      "   - Addition of epsilon: 1 operation per parameter\n",
      "   - Division by denominator: 1 operation per parameter\n",
      "   - Scale by learning rate: 1 operation per parameter\n",
      "   - Parameter update: 1 operation per parameter\n",
      "   - Total: 6 operations per parameter\n",
      "\n",
      "TOTAL PER PARAMETER:\n",
      "- Weight decay: 1 FLOP\n",
      "- First moment: 3 FLOPs\n",
      "- Second moment: 4 FLOPs\n",
      "- Parameter update: 6 FLOPs\n",
      "- Total per parameter: 14 FLOPs\n",
      "\n",
      "ALGEBRAIC EXPRESSION:\n",
      "If P is the total number of parameters:\n",
      "AdamW FLOPs = 14 * P\n",
      "\n",
      "For transformer with parameters P = 2*V*H + L*(16*H² + 2*H) + H:\n",
      "AdamW FLOPs = 14 * [2*V*H + L*(16*H² + 2*H) + H]\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# (c) AdamW FLOPs Analysis\n",
    "# ===============================================\n",
    "\n",
    "def analyze_adamw_flops():\n",
    "    \"\"\"\n",
    "    Analyze FLOPs required for one step of AdamW optimization.\n",
    "    \"\"\"\n",
    "    print(\"ADAMW FLOPS ANALYSIS\")\n",
    "    print(\"=\" * 25)\n",
    "    print(\"AdamW performs the following operations per parameter:\")\n",
    "    print()\n",
    "    \n",
    "    print(\"For each parameter p with gradient g:\")\n",
    "    print(\"1. Weight decay: p ← p * (1 - lr * weight_decay)\")\n",
    "    print(\"   - 1 multiplication per parameter\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. Update first moment: exp_avg ← β₁ * exp_avg + (1-β₁) * g\")\n",
    "    print(\"   - 3 operations per parameter (2 multiplications + 1 addition)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. Update second moment: exp_avg_sq ← β₂ * exp_avg_sq + (1-β₂) * g²\")\n",
    "    print(\"   - 4 operations per parameter (3 multiplications + 1 addition)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. Bias correction:\")\n",
    "    print(\"   - bias_correction1 = 1 - β₁^step\")\n",
    "    print(\"   - bias_correction2 = 1 - β₂^step\")\n",
    "    print(\"   - These are scalars computed once per step (negligible)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. Parameter update: p ← p - (lr / bias_correction1) * exp_avg / (√(exp_avg_sq / bias_correction2) + ε)\")\n",
    "    print(\"   - Square root: 1 operation per parameter\")\n",
    "    print(\"   - Division by bias_correction2: 1 operation per parameter\")\n",
    "    print(\"   - Addition of epsilon: 1 operation per parameter\")\n",
    "    print(\"   - Division by denominator: 1 operation per parameter\")\n",
    "    print(\"   - Scale by learning rate: 1 operation per parameter\")\n",
    "    print(\"   - Parameter update: 1 operation per parameter\")\n",
    "    print(\"   - Total: 6 operations per parameter\")\n",
    "    print()\n",
    "    \n",
    "    print(\"TOTAL PER PARAMETER:\")\n",
    "    print(\"- Weight decay: 1 FLOP\")\n",
    "    print(\"- First moment: 3 FLOPs\")\n",
    "    print(\"- Second moment: 4 FLOPs\") \n",
    "    print(\"- Parameter update: 6 FLOPs\")\n",
    "    print(\"- Total per parameter: 14 FLOPs\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ALGEBRAIC EXPRESSION:\")\n",
    "    print(\"If P is the total number of parameters:\")\n",
    "    print(\"AdamW FLOPs = 14 * P\")\n",
    "    print()\n",
    "    print(\"For transformer with parameters P = 2*V*H + L*(16*H² + 2*H) + H:\")\n",
    "    print(\"AdamW FLOPs = 14 * [2*V*H + L*(16*H² + 2*H) + H]\")\n",
    "\n",
    "analyze_adamw_flops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1jx7i3gmwjf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 XL MEMORY ANALYSIS\n",
      "==============================\n",
      "Configuration:\n",
      "- vocab_size = 50257\n",
      "- context_length = 1024\n",
      "- num_layers = 48\n",
      "- d_model = 1600\n",
      "- num_heads = 25\n",
      "- d_ff = 6400\n",
      "\n",
      "Memory breakdown for GPT-2 XL:\n",
      "1. Parameters + Gradients + Optimizer State (batch-independent):\n",
      "   Total parameters: 2,127,057,600\n",
      "   Memory: 136,131,686,400 bytes = 136.13 GB\n",
      "\n",
      "2. Activations (batch-dependent):\n",
      "   Coefficient: 18,040,889,344 bytes per batch item\n",
      "   Memory = 18040889344 * batch_size bytes\n",
      "   Memory = 18.040889 * batch_size GB\n",
      "\n",
      "FINAL EXPRESSION:\n",
      "Total Memory (GB) = 18.040889 * batch_size + 136.13\n",
      "Total Memory (GB) = 18.041 * batch_size + 136.1\n",
      "\n",
      "For 80GB memory limit:\n",
      "80 = 18.040889 * batch_size + 136.13\n",
      "batch_size = (80 - 136.13) / 18.040889\n",
      "batch_size = -3.1\n",
      "Maximum batch size: -3\n",
      "\n",
      "Verification: Memory at batch_size = -3:\n",
      "Memory = 18.041 * -3 + 136.1 = 82.0 GB\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# (b) GPT-2 XL Memory Analysis and Maximum Batch Size  \n",
    "# ===============================================\n",
    "\n",
    "def analyze_gpt2_xl_memory():\n",
    "    \"\"\"\n",
    "    Instantiate memory analysis for GPT-2 XL and find maximum batch size for 80GB memory.\n",
    "    \"\"\"\n",
    "    # GPT-2 XL configuration\n",
    "    vocab_size = 50257\n",
    "    context_length = 1024\n",
    "    num_layers = 48\n",
    "    d_model = 1600\n",
    "    num_heads = 25\n",
    "    d_ff = 4 * d_model  # 6400\n",
    "    \n",
    "    print(\"GPT-2 XL MEMORY ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"- vocab_size = {vocab_size}\")\n",
    "    print(f\"- context_length = {context_length}\")\n",
    "    print(f\"- num_layers = {num_layers}\")\n",
    "    print(f\"- d_model = {d_model}\")\n",
    "    print(f\"- num_heads = {num_heads}\")\n",
    "    print(f\"- d_ff = {d_ff}\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate batch-size independent terms\n",
    "    total_params = 2 * vocab_size * d_model + num_layers * (16 * d_model**2 + 2 * d_model) + d_model\n",
    "    param_grad_opt_memory = 16 * total_params * 4  # Parameters + gradients + optimizer state (in bytes)\n",
    "    \n",
    "    # Calculate batch-size dependent coefficient  \n",
    "    activation_per_batch = context_length * (2 * d_model + num_layers * (24 * d_model + 2 * num_heads * context_length) + 2 * vocab_size)\n",
    "    activation_coeff = 4 * activation_per_batch  # Convert to bytes\n",
    "    \n",
    "    print(\"Memory breakdown for GPT-2 XL:\")\n",
    "    print(f\"1. Parameters + Gradients + Optimizer State (batch-independent):\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Memory: {param_grad_opt_memory:,} bytes = {param_grad_opt_memory/1e9:.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"2. Activations (batch-dependent):\")\n",
    "    print(f\"   Coefficient: {activation_coeff:,} bytes per batch item\")\n",
    "    print(f\"   Memory = {activation_coeff} * batch_size bytes\")\n",
    "    print(f\"   Memory = {activation_coeff/1e9:.6f} * batch_size GB\")\n",
    "    print()\n",
    "    \n",
    "    print(\"FINAL EXPRESSION:\")\n",
    "    a = activation_coeff / 1e9  # Convert to GB\n",
    "    b = param_grad_opt_memory / 1e9  # Convert to GB\n",
    "    print(f\"Total Memory (GB) = {a:.6f} * batch_size + {b:.2f}\")\n",
    "    print(f\"Total Memory (GB) = {a:.3f} * batch_size + {b:.1f}\")\n",
    "    \n",
    "    # Find maximum batch size for 80GB\n",
    "    memory_limit_gb = 80\n",
    "    max_batch_size = (memory_limit_gb - b) / a\n",
    "    max_batch_size_int = int(max_batch_size)\n",
    "    \n",
    "    print()\n",
    "    print(f\"For 80GB memory limit:\")\n",
    "    print(f\"80 = {a:.6f} * batch_size + {b:.2f}\")\n",
    "    print(f\"batch_size = (80 - {b:.2f}) / {a:.6f}\")\n",
    "    print(f\"batch_size = {max_batch_size:.1f}\")\n",
    "    print(f\"Maximum batch size: {max_batch_size_int}\")\n",
    "    \n",
    "    # Verify the calculation\n",
    "    total_memory_at_max = a * max_batch_size_int + b\n",
    "    print(f\"\\nVerification: Memory at batch_size = {max_batch_size_int}:\")\n",
    "    print(f\"Memory = {a:.3f} * {max_batch_size_int} + {b:.1f} = {total_memory_at_max:.1f} GB\")\n",
    "    \n",
    "    return a, b, max_batch_size_int\n",
    "\n",
    "a, b, max_batch_size = analyze_gpt2_xl_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "mukqib04apa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALGEBRAIC EXPRESSIONS FOR ADAMW MEMORY USAGE\n",
      "==================================================\n",
      "Assumptions:\n",
      "- float32 tensors (4 bytes per element)\n",
      "- d_ff = 4 * d_model\n",
      "- Variables: batch_size (B), vocab_size (V), context_length (T),\n",
      "             num_layers (L), d_model (H), num_heads (A)\n",
      "\n",
      "1. PARAMETERS MEMORY:\n",
      "   - Token embeddings: V * H\n",
      "   - Per layer: 4*H² + 3*H*(4*H) + 2*H = 4*H² + 12*H² + 2*H = 16*H² + 2*H\n",
      "   - All layers: L * (16*H² + 2*H)\n",
      "   - Final layer norm: H\n",
      "   - LM head: V * H\n",
      "   - Total parameters: 2*V*H + L*(16*H² + 2*H) + H\n",
      "   - Parameters memory: 4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\n",
      "\n",
      "2. ACTIVATIONS MEMORY:\n",
      "   - Token embeddings: B * T * H\n",
      "   - Per layer breakdown:\n",
      "     * RMSNorm (2x): 2 * B * T * H\n",
      "     * Attention: 5*B*T*H + 2*B*A*T²\n",
      "     * FFN: 4*B*T*(4*H) + B*T*H = 17*B*T*H\n",
      "     * Per layer total: 2*B*T*H + 5*B*T*H + 2*B*A*T² + 17*B*T*H = 24*B*T*H + 2*B*A*T²\n",
      "   - All layers: L * (24*B*T*H + 2*B*A*T²)\n",
      "   - Final layer norm: B * T * H\n",
      "   - Output embedding: B * T * V\n",
      "   - Cross-entropy: B * T * V\n",
      "   - Total activations: B*T*H + L*(24*B*T*H + 2*B*A*T²) + B*T*H + 2*B*T*V\n",
      "   - Simplified: B*T*(2*H + L*(24*H + 2*A*T) + 2*V)\n",
      "   - Activations memory: 4 * B*T*(2*H + L*(24*H + 2*A*T) + 2*V) bytes\n",
      "\n",
      "3. GRADIENTS MEMORY:\n",
      "   - Same as parameters: 4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\n",
      "\n",
      "4. OPTIMIZER STATE MEMORY (AdamW):\n",
      "   - Two states per parameter (exp_avg + exp_avg_sq)\n",
      "   - Optimizer state memory: 8 * [2*V*H + L*(16*H² + 2*H) + H] bytes\n",
      "\n",
      "5. TOTAL MEMORY:\n",
      "   - Parameters: 4 * [2*V*H + L*(16*H² + 2*H) + H]\n",
      "   - Activations: 4 * B*T*(2*H + L*(24*H + 2*A*T) + 2*V)\n",
      "   - Gradients: 4 * [2*V*H + L*(16*H² + 2*H) + H]\n",
      "   - Optimizer: 8 * [2*V*H + L*(16*H² + 2*H) + H]\n",
      "   - TOTAL = 16*[2*V*H + L*(16*H² + 2*H) + H] + 4*B*T*(2*H + L*(24*H + 2*A*T) + 2*V) bytes\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# (a) General Memory Analysis for AdamW Training\n",
    "# ===============================================\n",
    "\n",
    "def analyze_memory_general():\n",
    "    \"\"\"\n",
    "    Provide algebraic expressions for AdamW memory usage.\n",
    "    \"\"\"\n",
    "    print(\"ALGEBRAIC EXPRESSIONS FOR ADAMW MEMORY USAGE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Assumptions:\")\n",
    "    print(\"- float32 tensors (4 bytes per element)\")\n",
    "    print(\"- d_ff = 4 * d_model\") \n",
    "    print(\"- Variables: batch_size (B), vocab_size (V), context_length (T),\")\n",
    "    print(\"             num_layers (L), d_model (H), num_heads (A)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"1. PARAMETERS MEMORY:\")\n",
    "    print(\"   - Token embeddings: V * H\")\n",
    "    print(\"   - Per layer: 4*H² + 3*H*(4*H) + 2*H = 4*H² + 12*H² + 2*H = 16*H² + 2*H\")\n",
    "    print(\"   - All layers: L * (16*H² + 2*H)\")\n",
    "    print(\"   - Final layer norm: H\")\n",
    "    print(\"   - LM head: V * H\")\n",
    "    print(\"   - Total parameters: 2*V*H + L*(16*H² + 2*H) + H\")\n",
    "    print(\"   - Parameters memory: 4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. ACTIVATIONS MEMORY:\")\n",
    "    print(\"   - Token embeddings: B * T * H\")\n",
    "    print(\"   - Per layer breakdown:\")\n",
    "    print(\"     * RMSNorm (2x): 2 * B * T * H\") \n",
    "    print(\"     * Attention: 5*B*T*H + 2*B*A*T²\")\n",
    "    print(\"     * FFN: 4*B*T*(4*H) + B*T*H = 17*B*T*H\")\n",
    "    print(\"     * Per layer total: 2*B*T*H + 5*B*T*H + 2*B*A*T² + 17*B*T*H = 24*B*T*H + 2*B*A*T²\")\n",
    "    print(\"   - All layers: L * (24*B*T*H + 2*B*A*T²)\")\n",
    "    print(\"   - Final layer norm: B * T * H\")\n",
    "    print(\"   - Output embedding: B * T * V\")\n",
    "    print(\"   - Cross-entropy: B * T * V\")\n",
    "    print(\"   - Total activations: B*T*H + L*(24*B*T*H + 2*B*A*T²) + B*T*H + 2*B*T*V\")\n",
    "    print(\"   - Simplified: B*T*(2*H + L*(24*H + 2*A*T) + 2*V)\")\n",
    "    print(\"   - Activations memory: 4 * B*T*(2*H + L*(24*H + 2*A*T) + 2*V) bytes\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. GRADIENTS MEMORY:\")\n",
    "    print(\"   - Same as parameters: 4 * [2*V*H + L*(16*H² + 2*H) + H] bytes\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. OPTIMIZER STATE MEMORY (AdamW):\")\n",
    "    print(\"   - Two states per parameter (exp_avg + exp_avg_sq)\")\n",
    "    print(\"   - Optimizer state memory: 8 * [2*V*H + L*(16*H² + 2*H) + H] bytes\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. TOTAL MEMORY:\")\n",
    "    print(\"   - Parameters: 4 * [2*V*H + L*(16*H² + 2*H) + H]\")\n",
    "    print(\"   - Activations: 4 * B*T*(2*H + L*(24*H + 2*A*T) + 2*V)\")\n",
    "    print(\"   - Gradients: 4 * [2*V*H + L*(16*H² + 2*H) + H]\")\n",
    "    print(\"   - Optimizer: 8 * [2*V*H + L*(16*H² + 2*H) + H]\")\n",
    "    print(\"   - TOTAL = 16*[2*V*H + L*(16*H² + 2*H) + H] + 4*B*T*(2*H + L*(24*H + 2*A*T) + 2*V) bytes\")\n",
    "\n",
    "analyze_memory_general()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ivx0kn02s1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# AdamW Memory and Compute Analysis\n",
    "# ===============================================\n",
    "\n",
    "def compute_memory_usage(batch_size: int, vocab_size: int, context_length: int, \n",
    "                        num_layers: int, d_model: int, num_heads: int, d_ff: int = None):\n",
    "    \"\"\"\n",
    "    Compute peak memory usage for AdamW training with float32 tensors.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Training batch size\n",
    "        vocab_size: Vocabulary size\n",
    "        context_length: Sequence length\n",
    "        num_layers: Number of transformer layers\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension (defaults to 4 * d_model)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory usage breakdown in bytes\n",
    "    \"\"\"\n",
    "    if d_ff is None:\n",
    "        d_ff = 4 * d_model\n",
    "    \n",
    "    # Each float32 takes 4 bytes\n",
    "    BYTES_PER_FLOAT32 = 4\n",
    "    \n",
    "    # ========================================\n",
    "    # PARAMETERS MEMORY\n",
    "    # ========================================\n",
    "    \n",
    "    # Token embeddings: vocab_size * d_model\n",
    "    token_emb_params = vocab_size * d_model\n",
    "    \n",
    "    # Per-layer parameters\n",
    "    # - Multi-head attention: 4 * d_model * d_model (Q, K, V, O projections)\n",
    "    # - FFN: 3 * d_model * d_ff (W1, W2, W3 for SwiGLU)\n",
    "    # - RMSNorm: 2 * d_model (attention + FFN normalization)\n",
    "    per_layer_params = 4 * d_model * d_model + 3 * d_model * d_ff + 2 * d_model\n",
    "    all_layers_params = num_layers * per_layer_params\n",
    "    \n",
    "    # Final layer norm: d_model\n",
    "    final_ln_params = d_model\n",
    "    \n",
    "    # LM head (output projection): vocab_size * d_model\n",
    "    lm_head_params = vocab_size * d_model\n",
    "    \n",
    "    total_params = token_emb_params + all_layers_params + final_ln_params + lm_head_params\n",
    "    parameters_memory = total_params * BYTES_PER_FLOAT32\n",
    "    \n",
    "    # ========================================\n",
    "    # ACTIVATIONS MEMORY (per forward pass)\n",
    "    # ========================================\n",
    "    \n",
    "    # Token embeddings output: batch_size * context_length * d_model\n",
    "    token_emb_activations = batch_size * context_length * d_model\n",
    "    \n",
    "    # Per-layer activations\n",
    "    per_layer_activations = 0\n",
    "    \n",
    "    # RMSNorm inputs and outputs (2 per layer): 2 * batch_size * context_length * d_model\n",
    "    rms_norm_activations = 2 * batch_size * context_length * d_model\n",
    "    \n",
    "    # Multi-head self-attention activations:\n",
    "    # - QKV projections: 3 * batch_size * context_length * d_model\n",
    "    # - Q^T K matrix: batch_size * num_heads * context_length * context_length\n",
    "    # - Softmax output: batch_size * num_heads * context_length * context_length\n",
    "    # - Attention output: batch_size * context_length * d_model\n",
    "    # - Output projection: batch_size * context_length * d_model\n",
    "    attention_activations = (3 * batch_size * context_length * d_model + \n",
    "                           2 * batch_size * num_heads * context_length * context_length +\n",
    "                           2 * batch_size * context_length * d_model)\n",
    "    \n",
    "    # Feed-forward activations:\n",
    "    # - W1 output: batch_size * context_length * d_ff\n",
    "    # - SiLU output: batch_size * context_length * d_ff  \n",
    "    # - W3 output: batch_size * context_length * d_ff\n",
    "    # - Element-wise product: batch_size * context_length * d_ff\n",
    "    # - W2 output: batch_size * context_length * d_model\n",
    "    ffn_activations = 4 * batch_size * context_length * d_ff + batch_size * context_length * d_model\n",
    "    \n",
    "    per_layer_activations = rms_norm_activations + attention_activations + ffn_activations\n",
    "    all_layers_activations = num_layers * per_layer_activations\n",
    "    \n",
    "    # Final layer norm: batch_size * context_length * d_model\n",
    "    final_ln_activations = batch_size * context_length * d_model\n",
    "    \n",
    "    # Output embedding: batch_size * context_length * vocab_size\n",
    "    output_emb_activations = batch_size * context_length * vocab_size\n",
    "    \n",
    "    # Cross-entropy computation (logits, softmax, loss): batch_size * context_length * vocab_size\n",
    "    cross_entropy_activations = batch_size * context_length * vocab_size\n",
    "    \n",
    "    total_activations = (token_emb_activations + all_layers_activations + \n",
    "                        final_ln_activations + output_emb_activations + cross_entropy_activations)\n",
    "    activations_memory = total_activations * BYTES_PER_FLOAT32\n",
    "    \n",
    "    # ========================================\n",
    "    # GRADIENTS MEMORY\n",
    "    # ========================================\n",
    "    \n",
    "    # Gradients have same size as parameters\n",
    "    gradients_memory = parameters_memory\n",
    "    \n",
    "    # ========================================\n",
    "    # OPTIMIZER STATE MEMORY (AdamW)\n",
    "    # ========================================\n",
    "    \n",
    "    # AdamW maintains two states per parameter:\n",
    "    # - exp_avg (first moment): same size as parameters\n",
    "    # - exp_avg_sq (second moment): same size as parameters\n",
    "    optimizer_state_memory = 2 * parameters_memory\n",
    "    \n",
    "    # ========================================\n",
    "    # TOTAL MEMORY\n",
    "    # ========================================\n",
    "    \n",
    "    total_memory = parameters_memory + activations_memory + gradients_memory + optimizer_state_memory\n",
    "    \n",
    "    return {\n",
    "        'parameters': parameters_memory,\n",
    "        'activations': activations_memory, \n",
    "        'gradients': gradients_memory,\n",
    "        'optimizer_state': optimizer_state_memory,\n",
    "        'total': total_memory,\n",
    "        'total_params': total_params\n",
    "    }\n",
    "\n",
    "def format_bytes(bytes_count: int) -> str:\n",
    "    \"\"\"Format bytes in human-readable units.\"\"\"\n",
    "    if bytes_count >= 1e9:\n",
    "        return f\"{bytes_count/1e9:.2f} GB\"\n",
    "    elif bytes_count >= 1e6:\n",
    "        return f\"{bytes_count/1e6:.2f} MB\"\n",
    "    elif bytes_count >= 1e3:\n",
    "        return f\"{bytes_count/1e3:.2f} KB\"\n",
    "    else:\n",
    "        return f\"{bytes_count} bytes\"\n",
    "\n",
    "def print_memory_breakdown(memory_dict: dict, title: str = \"Memory Breakdown\"):\n",
    "    \"\"\"Print formatted memory usage breakdown.\"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\" * len(title + \":\"))\n",
    "    \n",
    "    for key, value in memory_dict.items():\n",
    "        if key != 'total_params':\n",
    "            formatted_value = format_bytes(value)\n",
    "            print(f\"{key.replace('_', ' ').title()}: {formatted_value} ({value:,} bytes)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ccd6edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 XL parameter count: 2127057600\n",
      "GPT-2 XL parameter size with float32: 8508230400\n"
     ]
    }
   ],
   "source": [
    "def trainable_parameter_count_transformer_block(d_model: int, d_ff: int):\n",
    "    return 4 * d_model * d_model + 3 * d_model * d_ff + 2 * d_model\n",
    "\n",
    "def trainable_parameter_count_transformer_lm(vocab_size: int, context_length: int, num_layers: int,\n",
    "                                             d_model: int, num_heads: int, d_ff: int):\n",
    "    count = 0\n",
    "    count += 2 * vocab_size * d_model  # Token embedding + Output linear layer\n",
    "    count += d_model  # RMSNorm for output\n",
    "    count += trainable_parameter_count_transformer_block(d_model, d_ff) * num_layers\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "gpt2_xl_param_count = trainable_parameter_count_transformer_lm(vocab_size=50257, context_length=1024,\n",
    "                                                               num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
    "    \n",
    "print('GPT-2 XL parameter count: ' + str(gpt2_xl_param_count))\n",
    "print('GPT-2 XL parameter size with float32: ' + str(gpt2_xl_param_count * 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2w2jnenz72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# FLOP Calculation Functions\n",
    "# ===============================================\n",
    "\n",
    "def compute_attention_flops(batch_size: int, seq_len: int, d_model: int, num_heads: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for multi-head self-attention.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with FLOP counts for each attention operation\n",
    "    \"\"\"\n",
    "    head_dim = d_model // num_heads\n",
    "    \n",
    "    # QKV projections: 3 linear layers of [batch, seq_len, d_model] x [d_model, d_model]\n",
    "    qkv_flops = 3 * 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    # Attention scores: Q x K^T for each head\n",
    "    # Q, K shape per head: [batch, heads, seq_len, head_dim]\n",
    "    attention_scores_flops = 2 * batch_size * num_heads * seq_len * seq_len * head_dim\n",
    "    \n",
    "    # Attention output: AttentionWeights x V for each head\n",
    "    attention_output_flops = 2 * batch_size * num_heads * seq_len * seq_len * head_dim\n",
    "    \n",
    "    # Output projection: [batch, seq_len, d_model] x [d_model, d_model]\n",
    "    output_proj_flops = 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    return {\n",
    "        'qkv_projections': qkv_flops,\n",
    "        'attention_scores': attention_scores_flops,\n",
    "        'attention_output': attention_output_flops,\n",
    "        'output_projection': output_proj_flops,\n",
    "        'total_attention': qkv_flops + attention_scores_flops + attention_output_flops + output_proj_flops\n",
    "    }\n",
    "\n",
    "def compute_ffn_flops(batch_size: int, seq_len: int, d_model: int, d_ff: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for SwiGLU feed-forward network.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "        d_ff: Feed-forward dimension\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with FLOP counts for each FFN operation\n",
    "    \"\"\"\n",
    "    # W1 projection: [batch, seq_len, d_model] x [d_model, d_ff]\n",
    "    w1_flops = 2 * batch_size * seq_len * d_model * d_ff\n",
    "    \n",
    "    # W3 projection: [batch, seq_len, d_model] x [d_model, d_ff]\n",
    "    w3_flops = 2 * batch_size * seq_len * d_model * d_ff\n",
    "    \n",
    "    # W2 projection: [batch, seq_len, d_ff] x [d_ff, d_model]\n",
    "    w2_flops = 2 * batch_size * seq_len * d_ff * d_model\n",
    "    \n",
    "    return {\n",
    "        'w1_projection': w1_flops,\n",
    "        'w3_projection': w3_flops, \n",
    "        'w2_projection': w2_flops,\n",
    "        'total_ffn': w1_flops + w3_flops + w2_flops\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9mkftazoq2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformer_layer_flops(batch_size: int, seq_len: int, d_model: int, \n",
    "                                   num_heads: int, d_ff: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for a single transformer layer.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with FLOP counts for the transformer layer\n",
    "    \"\"\"\n",
    "    attention_flops = compute_attention_flops(batch_size, seq_len, d_model, num_heads)\n",
    "    ffn_flops = compute_ffn_flops(batch_size, seq_len, d_model, d_ff)\n",
    "    \n",
    "    # RMSNorm operations are relatively cheap (just element-wise ops), so we focus on matrix multiplies\n",
    "    \n",
    "    return {\n",
    "        'attention': attention_flops['total_attention'],\n",
    "        'ffn': ffn_flops['total_ffn'],\n",
    "        'total_layer': attention_flops['total_attention'] + ffn_flops['total_ffn'],\n",
    "        'attention_breakdown': attention_flops,\n",
    "        'ffn_breakdown': ffn_flops\n",
    "    }\n",
    "\n",
    "def compute_transformer_lm_flops(batch_size: int, seq_len: int, vocab_size: int,\n",
    "                                d_model: int, num_layers: int, num_heads: int, \n",
    "                                d_ff: int):\n",
    "    \"\"\"\n",
    "    Compute FLOPs for the complete transformer language model.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        seq_len: Sequence length\n",
    "        vocab_size: Vocabulary size\n",
    "        d_model: Model dimension\n",
    "        num_layers: Number of transformer layers\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with complete FLOP breakdown\n",
    "    \"\"\"\n",
    "    # Token embedding is just indexing (no matrix multiply)\n",
    "    embedding_flops = 0\n",
    "    \n",
    "    # Single layer FLOPs\n",
    "    layer_flops = compute_transformer_layer_flops(batch_size, seq_len, d_model, num_heads, d_ff)\n",
    "    \n",
    "    # All layers\n",
    "    all_layers_flops = num_layers * layer_flops['total_layer']\n",
    "    \n",
    "    # Language model head: [batch, seq_len, d_model] x [d_model, vocab_size]\n",
    "    lm_head_flops = 2 * batch_size * seq_len * d_model * vocab_size\n",
    "    \n",
    "    # Total FLOPs\n",
    "    total_flops = embedding_flops + all_layers_flops + lm_head_flops\n",
    "    \n",
    "    return {\n",
    "        'embedding': embedding_flops,\n",
    "        'per_layer': layer_flops['total_layer'],\n",
    "        'all_layers': all_layers_flops,\n",
    "        'lm_head': lm_head_flops,\n",
    "        'total': total_flops,\n",
    "        'layer_breakdown': layer_flops\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8wdviveb4on",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_flops(flops: int) -> str:\n",
    "    \"\"\"\n",
    "    Format FLOP count in human-readable format.\n",
    "    \n",
    "    Args:\n",
    "        flops: Number of FLOPs\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with appropriate units\n",
    "    \"\"\"\n",
    "    if flops >= 1e12:\n",
    "        return f\"{flops/1e12:.1f}T\"\n",
    "    elif flops >= 1e9:\n",
    "        return f\"{flops/1e9:.1f}B\"\n",
    "    elif flops >= 1e6:\n",
    "        return f\"{flops/1e6:.1f}M\"\n",
    "    elif flops >= 1e3:\n",
    "        return f\"{flops/1e3:.1f}K\"\n",
    "    else:\n",
    "        return str(flops)\n",
    "\n",
    "def print_flop_breakdown(flop_dict, title: str = \"FLOP Breakdown\"):\n",
    "    \"\"\"\n",
    "    Print a nicely formatted FLOP breakdown.\n",
    "    \n",
    "    Args:\n",
    "        flop_dict: Dictionary with FLOP counts\n",
    "        title: Title for the breakdown\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\" * len(title + \":\"))\n",
    "    \n",
    "    for key, value in flop_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Skip nested breakdown dictionaries in the main view\n",
    "            if 'breakdown' in key:\n",
    "                continue\n",
    "            print(f\"{key.replace('_', ' ').title()}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                if isinstance(sub_value, (int, float)):\n",
    "                    print(f\"  {sub_key.replace('_', ' ').title()}: {format_flops(sub_value)} ({sub_value:,})\")\n",
    "        else:\n",
    "            print(f\"{key.replace('_', ' ').title()}: {format_flops(value)} ({value:,})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dytjc94elf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_breakdown(flop_dict, title: str = \"Detailed FLOP Breakdown\"):\n",
    "    \"\"\"\n",
    "    Print a detailed FLOP breakdown including all nested components.\n",
    "    \n",
    "    Args:\n",
    "        flop_dict: Dictionary with FLOP counts\n",
    "        title: Title for the breakdown\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\" * len(title + \":\"))\n",
    "    \n",
    "    # Main totals first\n",
    "    for key, value in flop_dict.items():\n",
    "        if not isinstance(value, dict) and 'breakdown' not in key:\n",
    "            print(f\"{key.replace('_', ' ').title()}: {format_flops(value)} ({value:,})\")\n",
    "    \n",
    "    # Then detailed breakdowns\n",
    "    if 'layer_breakdown' in flop_dict:\n",
    "        layer_breakdown = flop_dict['layer_breakdown']\n",
    "        print(f\"\\nPer-Layer Breakdown:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        if 'attention_breakdown' in layer_breakdown:\n",
    "            print(\"  Attention Components:\")\n",
    "            for k, v in layer_breakdown['attention_breakdown'].items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    print(f\"    {k.replace('_', ' ').title()}: {format_flops(v)} ({v:,})\")\n",
    "        \n",
    "        if 'ffn_breakdown' in layer_breakdown:\n",
    "            print(\"  FFN Components:\")\n",
    "            for k, v in layer_breakdown['ffn_breakdown'].items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    print(f\"    {k.replace('_', ' ').title()}: {format_flops(v)} ({v:,})\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5xgi4g2ffug",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 XL FLOP Breakdown:\n",
      "========================\n",
      "Embedding: 0 (0)\n",
      "Per Layer: 90.6B (90,596,966,400)\n",
      "All Layers: 4.3T (4,348,654,387,200)\n",
      "Lm Head: 164.7B (164,682,137,600)\n",
      "Total: 4.5T (4,513,336,524,800)\n",
      "\n",
      "Total FLOPs: 4.5T\n",
      "FLOPs per token: 4.4B\n",
      "\n",
      "GPT-2 XL Detailed Breakdown:\n",
      "============================\n",
      "Embedding: 0 (0)\n",
      "Per Layer: 90.6B (90,596,966,400)\n",
      "All Layers: 4.3T (4,348,654,387,200)\n",
      "Lm Head: 164.7B (164,682,137,600)\n",
      "Total: 4.5T (4,513,336,524,800)\n",
      "\n",
      "Per-Layer Breakdown:\n",
      "--------------------\n",
      "  Attention Components:\n",
      "    Qkv Projections: 15.7B (15,728,640,000)\n",
      "    Attention Scores: 3.4B (3,355,443,200)\n",
      "    Attention Output: 3.4B (3,355,443,200)\n",
      "    Output Projection: 5.2B (5,242,880,000)\n",
      "    Total Attention: 27.7B (27,682,406,400)\n",
      "  FFN Components:\n",
      "    W1 Projection: 21.0B (20,971,520,000)\n",
      "    W3 Projection: 21.0B (20,971,520,000)\n",
      "    W2 Projection: 21.0B (20,971,520,000)\n",
      "    Total Ffn: 62.9B (62,914,560,000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Example: GPT-2 XL FLOP Analysis\n",
    "# ===============================================\n",
    "\n",
    "# GPT-2 XL Configuration\n",
    "gpt2_xl_config = {\n",
    "    'batch_size': 1,\n",
    "    'seq_len': 1024,\n",
    "    'vocab_size': 50257,\n",
    "    'd_model': 1600,\n",
    "    'num_layers': 48,\n",
    "    'num_heads': 25,\n",
    "    'd_ff': 6400\n",
    "}\n",
    "\n",
    "# Compute FLOPs for GPT-2 XL\n",
    "gpt2_xl_flops = compute_transformer_lm_flops(**gpt2_xl_config)\n",
    "\n",
    "# Print breakdown\n",
    "print_flop_breakdown(gpt2_xl_flops, \"GPT-2 XL FLOP Breakdown\")\n",
    "\n",
    "print(f\"Total FLOPs: {format_flops(gpt2_xl_flops['total'])}\")\n",
    "print(f\"FLOPs per token: {format_flops(gpt2_xl_flops['total'] // gpt2_xl_config['seq_len'])}\")\n",
    "\n",
    "# Show detailed component breakdown\n",
    "print_detailed_breakdown(gpt2_xl_flops, \"GPT-2 XL Detailed Breakdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
